{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import branch_length\n",
    "import distributions\n",
    "import optimizers\n",
    "import sbn\n",
    "\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "alt.renderers.enable(\"notebook\")\n",
    "#def my_theme(*args, **kwargs):\n",
    "#    return {\"background\": \"white\"}\n",
    "          \n",
    "#alt.themes.register('my_theme', my_theme)\n",
    "#alt.themes.enable('my_theme')\n",
    "\n",
    "import importlib \n",
    "importlib.reload(branch_length)\n",
    "importlib.reload(distributions)\n",
    "importlib.reload(optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst = sbn.instance(\"charlie\")\n",
    "inst.tree_collection = sbn.TreeCollection(\n",
    "    [sbn.Tree.of_parent_id_vector([3, 3, 3])],\n",
    "    [\"mars\", \"saturn\", \"jupiter\"])\n",
    "inst.read_fasta_file('../data/hello.fasta')\n",
    "inst.make_beagle_instances(2)\n",
    "branch_lengths = np.array(inst.tree_collection.trees[0].branch_lengths,\n",
    "                          copy=False)\n",
    "branch_lengths[:] = np.array([0.1, 0.1, 0.3, 0.])\n",
    "\n",
    "def log_like_with(branch_id: int, branch_length: float, grad=False):\n",
    "    saved_branch_length = branch_lengths[branch_id]\n",
    "    branch_lengths[branch_id] = branch_length\n",
    "    if grad:\n",
    "        _, log_grad = inst.branch_gradients()[0]\n",
    "        result = np.array(log_grad)[branch_id]\n",
    "    else:\n",
    "        result = np.array(inst.log_likelihoods())[0]\n",
    "    branch_lengths[branch_id] = saved_branch_length\n",
    "    return result\n",
    "\n",
    "def phylo_log_like(x_arr):\n",
    "    return np.array([log_like_with(2, x) for x in x_arr])\n",
    "\n",
    "def phylo_log_like_grad(x_arr):\n",
    "    return np.array([[log_like_with(2, x, grad=True)] for x in x_arr])\n",
    "\n",
    "x_vals = np.linspace(0, 0.3, 100)\n",
    "df = pd.DataFrame({\"x\": x_vals, \"y\": phylo_log_like(x_vals)})\n",
    "alt.Chart(df).mark_line().encode(\n",
    "        alt.X(\"x\"),\n",
    "        alt.Y(\"y\", scale=alt.Scale(zero=False))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake \"phylogenetic\" distributions.\n",
    "if False:\n",
    "    d = distributions.LogNormal(1)\n",
    "    true_loc = np.array([-2.3])\n",
    "    true_shape = np.array([-0.6])\n",
    "    phylo_log_like = lambda x: d.log_prob(x, true_loc, true_shape)\n",
    "    phylo_log_like_grad = lambda x: d.log_prob_grad(x, true_loc, true_shape)\n",
    "if False:\n",
    "    d = distributions.Gamma(1)\n",
    "    alpha = np.array([2.])\n",
    "    beta = np.array([5.])\n",
    "    phylo_log_like = lambda x: d.log_prob(x, alpha, beta)\n",
    "    phylo_log_like_grad = lambda x: d.log_prob_grad(x, alpha, beta) * np.exp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are values for plotting and for calculating KL divergence.\n",
    "x_vals = np.linspace(0.01, 0.7, 100)\n",
    "x_vals_transpose = np.transpose(np.array([x_vals]))\n",
    "\n",
    "transforms = {\n",
    "    \"identity\": lambda x: x,\n",
    "    \"normalize\": lambda x: x / np.sum(x),\n",
    "    \"exp\": lambda x: np.exp(x),\n",
    "    \"exp_normalize\": lambda x: np.exp(x) / sum(np.exp(x))\n",
    "}\n",
    "\n",
    "def plot_functions(f_true, f_approx, transform=\"identity\"):\n",
    "    transform = transforms[transform]\n",
    "    data = pd.DataFrame({\"x\": x_vals, \"truth\": transform(f_true(x_vals_transpose)), \n",
    "                         \"approx\": transform(f_approx(x_vals_transpose))})\n",
    "    return alt.Chart(data.melt(id_vars=[\"x\"])).mark_line().encode(\n",
    "        x='x',\n",
    "        y='value',\n",
    "        color='variable'\n",
    "    )\n",
    "\n",
    "def kl_div(f_true, f_approx, transform=\"identity\"):\n",
    "    transform = transforms[transform]\n",
    "    f_approx_trans = lambda x: transform(f_approx(x))\n",
    "    f_true_trans = lambda x: transform(f_true(x))\n",
    "    return {\n",
    "        \"standard\": stats.entropy(f_true_trans(x_vals_transpose), f_approx_trans(x_vals_transpose)),\n",
    "        \"reversed\": stats.entropy(f_approx_trans(x_vals_transpose), f_true_trans(x_vals_transpose))}\n",
    "\n",
    "def elbo(f_true, f_approx):\n",
    "    approx_log_likes = f_approx(x_vals_transpose)\n",
    "    approx_log_probs = np.exp(approx_log_likes) / np.sum(np.exp(approx_log_likes))\n",
    "    elbo = np.sum(approx_log_probs * (phylo_log_like(x_vals_transpose) - approx_log_likes))\n",
    "    return {\"elbo\": elbo}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branch_length_param_count = 1\n",
    "sgd_server_args = {'loc': branch_length_param_count, 'shape': branch_length_param_count}\n",
    "infer_opt = optimizers.SGD_Server(sgd_server_args)\n",
    "stepsz = 0.005\n",
    "clip = 5.\n",
    "anneal_freq = 500\n",
    "anneal_rate = 0.95\n",
    "# measured_divergence = lambda: kl_div(phylo_log_like, q_log_like, transform=\"exp_normalize\")\n",
    "measured_divergence = lambda: elbo(phylo_log_like, q_log_like)\n",
    "\n",
    "# Variational distribution\n",
    "q = distributions.LogNormal(1)\n",
    "loc = np.array([1.])\n",
    "shape = np.array([0.5])\n",
    "q_log_like = lambda x: q.log_prob(x, loc, shape)\n",
    "\n",
    "def multi_elbo_grad(x, loc, shape, clip):\n",
    "    weights = branch_length.like_weights(q, phylo_log_like(x), x, loc, shape, clip)\n",
    "    return branch_length.param_grad(q, weights, phylo_log_like_grad(x), x, loc, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_multi_elbo_grad(x, loc, shape, clip):\n",
    "    true_distribution = tfp.distributions.Gamma(\n",
    "        concentration=alpha[0], rate=beta[0]\n",
    "    )\n",
    "    # A variety of epsilons.\n",
    "    epsilon = tf.constant(np.random.normal(0., 1., 100))\n",
    "    with tf.GradientTape() as g:\n",
    "        tf_loc = tf.constant(loc)\n",
    "        tf_scale = tf.constant(shape)\n",
    "        g.watch(tf_loc)\n",
    "        g.watch(tf_scale)\n",
    "        tf_x = tf.math.exp(tf_loc + tf_scale * epsilon)\n",
    "        # This is the log of the full sum of ratios as in the equation just before (7)\n",
    "        # in the 2018 ICLR paper.\n",
    "        y = tf.math.log(\n",
    "            # In principle we should have a 1/K term here, but it disappears in the log\n",
    "            # grad.\n",
    "            tf.math.reduce_sum(\n",
    "                true_distribution.prob(tf_x)\n",
    "                / tfp.distributions.LogNormal(loc=tf_loc, scale=tf_scale).prob(tf_x)\n",
    "            )\n",
    "        )\n",
    "        x_arr = np.array([tf_x.numpy()]).transpose()\n",
    "        tf_gradient = [grad.numpy() for grad in g.gradient(y, [tf_loc, tf_scale])]\n",
    "    return tf_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step(sample_count):\n",
    "    global loc, shape\n",
    "    x = q.sample(loc, shape, sample_count)\n",
    "    loc_grad, shape_grad = tf_multi_elbo_grad(x, loc, shape, clip)\n",
    "    if clip:\n",
    "        loc_grad = np.clip(loc_grad, -clip, clip)\n",
    "        shape_grad = np.clip(shape_grad, -clip, clip)\n",
    "    update_dict = infer_opt.adam(stepsz_dict, {'loc': loc, 'shape': shape}, \n",
    "                                 {'loc': loc_grad, 'shape': shape_grad})\n",
    "    loc += update_dict['loc']\n",
    "    shape += update_dict['shape']\n",
    "    return measured_divergence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = np.array([-0.6])\n",
    "shape = np.array([1.1])\n",
    "measured_divergence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = np.array([-1.6])\n",
    "shape = np.array([1.1])\n",
    "measured_divergence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_functions(phylo_log_like, q_log_like, transform=\"exp_normalize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = q.sample(loc, shape, 100)\n",
    "multi_elbo_grad(x, loc, shape, clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = q.sample(loc, shape, 100)\n",
    "#x = np.array([np.exp(loc+0.05)])\n",
    "loc_grad, shape_grad = multi_elbo_grad(x, loc, shape, clip)\n",
    "loc_grad = np.clip(loc_grad, -clip, clip)\n",
    "shape_grad = np.clip(shape_grad, -clip, clip)\n",
    "print(loc, loc_grad, shape, shape_grad)\n",
    "loc += stepsz*loc_grad\n",
    "shape += stepsz*shape_grad\n",
    "measured_divergence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_samples = q.sample(loc, shape, 1000)\n",
    "\n",
    "gradient_samples = pd.DataFrame({\n",
    "    \"x\": [multi_elbo_grad(np.array([[x]]), loc, shape, clip)[0][0] for x in x_samples.transpose()[0]]})\n",
    "gradient_samples.loc[gradient_samples['x'] < -5,] = -5\n",
    "\n",
    "alt.Chart(gradient_samples).mark_bar().encode(\n",
    "    alt.X(\"x\", bin=alt.Bin(maxbins=50)),\n",
    "    y='count()',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_samples.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lognormal_samples = pd.DataFrame({\"x\": q.sample(loc, shape, 5000).transpose()[0]})\n",
    "lognormal_samples = lognormal_samples.loc[lognormal_samples['x'] < 0.70,]\n",
    "\n",
    "alt.Chart(lognormal_samples).mark_bar().encode(\n",
    "    alt.X(\"x\", bin=alt.Bin(maxbins=50)),\n",
    "    y='count()',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_grad_data = pd.DataFrame({\n",
    "    \"x\": x_vals, \n",
    "    \"loc_grad\": [multi_elbo_grad(np.array([[x]]), loc, shape, clip)[0][0] for x in x_vals],\n",
    "    \"pdf\": q_log_like(x_vals_transpose)\n",
    "})\n",
    "base = alt.Chart(loc_grad_data).mark_line().encode(\n",
    "        alt.X(\"x\"),\n",
    "        alt.Y(\"loc_grad\", scale=alt.Scale(zero=False))\n",
    "    )\n",
    "base + base.mark_line(color=\"red\").encode(y=\"pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_functions(phylo_log_like, q_log_like, transform=\"exp_normalize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize the SGD server.\n",
    "infer_opt.__init__(sgd_server_args)\n",
    "stepsz_dict = {'loc': stepsz, 'shape': stepsz}\n",
    "loc = np.array([-1.])\n",
    "shape = np.array([1.])\n",
    "results = [measured_divergence()]\n",
    "\n",
    "for i in range(2000):\n",
    "    results.append(gradient_step(100))\n",
    "    if i % anneal_freq == 0:\n",
    "        for k in stepsz_dict:\n",
    "            stepsz_dict[k] *= anneal_rate\n",
    "\n",
    "\n",
    "plot_data = pd.DataFrame(results).reset_index()\n",
    "    \n",
    "alt.Chart(\n",
    "    pd.melt(plot_data, id_vars=[\"index\"], var_name=\"measured divergence\", value_name=\"divergence\")\n",
    "    ).mark_line().encode(\n",
    "        alt.X(\"index\"),\n",
    "        alt.Y(\"divergence\",\n",
    "              scale=alt.Scale()),\n",
    "        color=\"measured divergence\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_functions(phylo_log_like, q_log_like, transform=\"exp_normalize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gradients(particle_count, sample_count):\n",
    "    def sample_gradient():\n",
    "        x = q.sample(loc, shape, 1)\n",
    "        loc_grad, shape_grad = multi_elbo_grad(x, loc, shape, clip)\n",
    "        pcs = str(particle_count)\n",
    "        return {\"loc_grad_\"+pcs: loc_grad[0], \"shape_grad_\"+pcs: shape_grad[0]}\n",
    "    return pd.DataFrame([sample_gradient() for _ in range(sample_count)])\n",
    "\n",
    "sample_count = 1000\n",
    "raw = pd.concat([sample_gradients(1, sample_count), sample_gradients(10, sample_count)], axis=1)\n",
    "\n",
    "raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
