{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "tf.enable_v2_behavior()\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optimizers\n",
    "import sbn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following can be used in place of the phylogenetic likelihood for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 2.\n",
    "beta = 5.\n",
    "gamma = tfd.Gamma(concentration=alpha, rate=beta)\n",
    "\n",
    "def grad_log_like(x):\n",
    "    with tf.GradientTape() as g:\n",
    "        tf_x = tf.constant(x, dtype=tf.float32)\n",
    "        g.watch(tf_x)\n",
    "        return g.gradient(gamma.log_prob(tf_x), tf_x).numpy()\n",
    "\n",
    "def log_like(x):\n",
    "    return gamma.log_prob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst = sbn.instance(\"charlie\")\n",
    "inst.tree_collection = sbn.TreeCollection(\n",
    "    [sbn.Tree.of_parent_id_vector([3, 3, 3])],\n",
    "    [\"mars\", \"saturn\", \"jupiter\"])\n",
    "inst.read_fasta_file('../data/hello.fasta')\n",
    "inst.make_beagle_instances(2)\n",
    "branch_lengths_extended = np.array(inst.tree_collection.trees[0].branch_lengths,\n",
    "                          copy=False)\n",
    "branch_lengths_extended[:] = np.array([0.1, 0.1, 0.3, 0.])\n",
    "# Here we are getting a slice that excludes the last (fake) element. \n",
    "# Thus we can just deal with the actual branch lengths.\n",
    "branch_lengths = branch_lengths_extended[:len(branch_lengths_extended)-1]\n",
    "\n",
    "def log_like_with(in_branch_lengths, grad=False):\n",
    "    global branch_lengths\n",
    "    saved_branch_lengths = branch_lengths.copy()\n",
    "    branch_lengths[:] = in_branch_lengths\n",
    "    if grad:\n",
    "        _, log_grad = inst.branch_gradients()[0]\n",
    "        result = np.array(log_grad)\n",
    "    else:\n",
    "        result = np.array(inst.log_likelihoods())[0]\n",
    "        branch_lengths[:] = saved_branch_lengths\n",
    "    return result\n",
    "\n",
    "def phylo_log_like(x_arr):\n",
    "    return np.apply_along_axis(log_like_with, 1, x_arr)\n",
    "\n",
    "def grad_phylo_log_like(x_arr):\n",
    "    return np.apply_along_axis(lambda x: log_like_with(x, grad=True), 1, x_arr)[:,:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFContinuousParameterModel:\n",
    "    def __init__(self, q_factory, initial_params, branch_count, particle_count, step_size=0.01):\n",
    "        assert initial_params.ndim == 1\n",
    "        self.q_factory = q_factory\n",
    "        self.param_matrix = np.full((branch_count, len(initial_params)), initial_params)\n",
    "        #self.param_matrix = np.copy(initial_params)\n",
    "        self.particle_count = particle_count\n",
    "        self.step_size = step_size\n",
    "        # The current stored sample.\n",
    "        self.x = None\n",
    "        # The gradient of x with respect to the parameters of q.\n",
    "        self.grad_x = None\n",
    "        # The stochastic gradient of log sum q for x.\n",
    "        self.grad_log_sum_q = None\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def branch_count(self):\n",
    "        return self.param_matrix.shape[0]\n",
    "    \n",
    "\n",
    "    @property\n",
    "    def param_count(self):\n",
    "        return self.param_matrix.shape[1]\n",
    "\n",
    "        \n",
    "    def sample(self):\n",
    "        with tf.GradientTape(persistent=True) as g:\n",
    "            tf_params = tf.constant(self.param_matrix, dtype=tf.float32)\n",
    "            g.watch(tf_params)\n",
    "            q_distribution = self.q_factory(tf_params)\n",
    "            tf_x = q_distribution.sample(self.particle_count)\n",
    "            q_term = tf.math.reduce_sum(tf.math.log(q_distribution.prob(tf_x)))\n",
    "        self.x = tf_x.numpy()\n",
    "        # The Jacobian is laid out as particles x edges x edges x params.\n",
    "        self.grad_x = np.sum(g.jacobian(tf_x, tf_params).numpy(), axis=2)\n",
    "        self.grad_log_sum_q = g.gradient(q_term, tf_params).numpy()\n",
    "        del g  # Should happen anyway but being explicit to remember.\n",
    "        return self.x\n",
    "    \n",
    "    \n",
    "    def clear_sample(self):\n",
    "        self.x = None\n",
    "        self.grad_x = None\n",
    "        self.grad_log_sum_q = None\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def _chain_rule(grad_log_p_x, grad_x):\n",
    "        return np.tensordot(grad_log_p_x.transpose(), grad_x, axes=1).diagonal().transpose()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _slow_chain_rule(grad_log_p_x, grad_x):\n",
    "        particle_count, branch_count, param_count = grad_x.shape\n",
    "        result = np.zeros((branch_count, param_count))\n",
    "        for branch in range(branch_count):\n",
    "            for param in range(param_count):\n",
    "                for particle in range(particle_count):\n",
    "                    result[branch, param] += grad_log_p_x[particle, branch] * grad_x[particle, branch, param] \n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def linspace_one_branch(self, branch, min_x, max_x, num=50, default=0.1):\n",
    "        \"\"\"\n",
    "        Fill a num x branch_count array with default, except for the branch column,\n",
    "        which gets filled with a linspace.\n",
    "        \"\"\"\n",
    "        a = np.full((num,self.branch_count), default)\n",
    "        a[:,branch] = np.linspace(min_x, max_x, num)\n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def elbo_gradient_using_current_sample(self, grad_log_p_x):\n",
    "        assert self.grad_x is not None\n",
    "        assert np.allclose(\n",
    "            self._chain_rule(grad_log_p_x, self.grad_x), \n",
    "            self._slow_chain_rule(grad_log_p_x, self.grad_x))\n",
    "        unnormalized_result = self._chain_rule(grad_log_p_x, self.grad_x) - self.grad_log_sum_q\n",
    "        return unnormalized_result / self.particle_count\n",
    "    \n",
    "    \n",
    "    def gradient_step(self, grad_log_p_x, history = None):\n",
    "        grad = self.elbo_gradient_using_current_sample(grad_log_p_x)\n",
    "        self.param_matrix += self.step_size * grad\n",
    "        self.clear_sample()\n",
    "        if history is not None:\n",
    "            history.append(self.param_matrix.copy())\n",
    "    \n",
    "    \n",
    "    def plot_1d(self, ax, target_log_like, which_branch, max_x = 0.5):\n",
    "        min_x = max_x/100\n",
    "        x_vals = m.linspace_one_branch(which_branch, min_x, max_x, 100)\n",
    "        q_distribution = self.q_factory(self.param_matrix)\n",
    "        df = pd.DataFrame({\n",
    "            \"x\": x_vals[:,which_branch], \n",
    "            \"target\": sp.special.softmax(target_log_like(x_vals)),\n",
    "            \"fit\": sp.special.softmax(q_distribution.log_prob(x_vals).numpy()[:,which_branch])})\n",
    "        return df.plot(ax=ax, x=\"x\", y=[\"target\", \"fit\"], kind=\"line\", \n",
    "                title=q_distribution._name+\" \"+str(self.param_matrix[which_branch,:]))\n",
    "\n",
    "    \n",
    "    def plot(self, target_log_like, max_x = 0.5):\n",
    "        f, axarr = plt.subplots(self.branch_count, sharex=True)\n",
    "        for which_branch in range(self.branch_count):\n",
    "             self.plot_1d(axarr[which_branch], target_log_like, which_branch, max_x = 0.5)\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_factory(params):\n",
    "    return tfp.distributions.Exponential(rate=params[:,0])\n",
    "\n",
    "def gamma_factory(params):\n",
    "    return tfp.distributions.Gamma(concentration=params[:,0], rate=params[:,1])\n",
    "\n",
    "def inverse_gamma_factory(params):\n",
    "    return tfp.distributions.InverseGamma(concentration=params[:,0], scale=params[:,1])\n",
    "\n",
    "def lognormal_factory(params):\n",
    "    return tfp.distributions.LogNormal(loc=params[:,0], scale=params[:,1])\n",
    "\n",
    "#m = TFContinuousParameterModel(inverse_gamma_factory, np.array([2., 0.5]), 3, 100, step_size=0.05)\n",
    "m = TFContinuousParameterModel(gamma_factory, np.array([2., 12.]), 3, 100, step_size=0.05)\n",
    "#m = TFContinuousParameterModel(lognormal_factory, np.array([-2., 0.5]), 3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot(phylo_log_like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = []\n",
    "for _ in range(100):\n",
    "    m.sample()\n",
    "    m.gradient_step(grad_phylo_log_like(m.x), history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot(phylo_log_like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
