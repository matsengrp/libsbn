{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "%matplotlib inline\n",
    "#%config InlineBackend.figure_formats = ['svg']\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "tf.enable_v2_behavior()\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import continuous_parameter_models\n",
    "import optimizers\n",
    "import sbn\n",
    "\n",
    "import importlib \n",
    "importlib.reload(continuous_parameter_models)\n",
    "importlib.reload(optimizers)\n",
    "importlib.reload(sbn)\n",
    "\n",
    "from continuous_parameter_models import TFContinuousParameterModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following can be used in place of the phylogenetic likelihood for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 2.\n",
    "beta = 5.\n",
    "gamma = tfd.Gamma(concentration=alpha, rate=beta)\n",
    "\n",
    "def grad_log_like(x):\n",
    "    with tf.GradientTape() as g:\n",
    "        tf_x = tf.constant(x, dtype=tf.float32)\n",
    "        g.watch(tf_x)\n",
    "        return g.gradient(gamma.log_prob(tf_x), tf_x).numpy()\n",
    "\n",
    "def log_like(x):\n",
    "    return gamma.log_prob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst = sbn.instance(\"charlie\")\n",
    "data = \"primates\"\n",
    "if data == \"DS1\":\n",
    "    inst.read_newick_file(\"../data/ds1.raxml.tre\")\n",
    "    inst.read_fasta_file(\"../data/DS1.fasta\")\n",
    "    max_x = 0.02\n",
    "elif data == \"primates\":\n",
    "    inst.read_newick_file(\"../data/primates.tre\")\n",
    "    inst.read_fasta_file(\"../data/primates.fasta\")\n",
    "    max_x = 0.2\n",
    "elif data == \"hello\":\n",
    "    inst.tree_collection = sbn.TreeCollection(\n",
    "        [sbn.Tree.of_parent_id_vector([3, 3, 3])],\n",
    "        [\"mars\", \"saturn\", \"jupiter\"])\n",
    "    inst.read_fasta_file('../data/hello.fasta')\n",
    "    max_x = 0.4\n",
    "else:\n",
    "    assert False\n",
    "\n",
    "inst.make_beagle_instances(1)\n",
    "branch_lengths_extended = np.array(inst.tree_collection.trees[0].branch_lengths,\n",
    "                          copy=False)\n",
    "# Here we are getting a slice that excludes the last (fake) element. \n",
    "# Thus we can just deal with the actual branch lengths.\n",
    "branch_lengths = branch_lengths_extended[:len(branch_lengths_extended)-1]\n",
    "\n",
    "if data == \"hello\":\n",
    "    branch_lengths_extended[:] = np.array([0.2, 0.07, 0.07, 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_like_with(in_branch_lengths, grad=False):\n",
    "    global branch_lengths\n",
    "    saved_branch_lengths = branch_lengths.copy()\n",
    "    branch_lengths[:] = in_branch_lengths\n",
    "    if grad:\n",
    "        _, log_grad = inst.branch_gradients()[0]\n",
    "        result = np.array(log_grad)\n",
    "    else:\n",
    "        result = np.array(inst.log_likelihoods())[0]\n",
    "        branch_lengths[:] = saved_branch_lengths\n",
    "    return result\n",
    "\n",
    "def phylo_log_like(x_arr):\n",
    "    \"\"\"\n",
    "    Calculate phylogenetic log likelihood for each of the branch length\n",
    "    assignments laid out along axis 1.\n",
    "    \"\"\"\n",
    "    return np.apply_along_axis(log_like_with, 1, x_arr)\n",
    "\n",
    "def grad_phylo_log_like(x_arr):\n",
    "    return np.apply_along_axis(lambda x: log_like_with(x, grad=True), 1, x_arr)[:,:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_factory(params):\n",
    "    return tfd.Exponential(rate=params[:,0])\n",
    "\n",
    "def gamma_factory(params):\n",
    "    return tfd.Gamma(concentration=params[:,0], rate=params[:,1])\n",
    "\n",
    "def inverse_gamma_factory(params):\n",
    "    return tfd.InverseGamma(concentration=params[:,0], scale=params[:,1])\n",
    "\n",
    "def lognormal_factory(params):\n",
    "    return tfd.LogNormal(loc=params[:,0], scale=params[:,1])\n",
    "\n",
    "def truncated_lognormal_factory(params):\n",
    "    exp_shift = tfp.bijectors.Chain(\n",
    "        [tfp.bijectors.AffineScalar(shift=-params[:,2]), tfp.bijectors.Exp()])\n",
    "    return tfd.TransformedDistribution(\n",
    "        distribution=tfd.TruncatedNormal(\n",
    "            loc=params[:,0], \n",
    "            scale=params[:,1], \n",
    "            low=tf.math.log(params[:,2]), high=999), \n",
    "        bijector=exp_shift, \n",
    "        name=\"TruncatedLogNormal\")\n",
    "\n",
    "m = TFContinuousParameterModel(gamma_factory, np.array([4., 20.]), len(branch_lengths), 100)\n",
    "#m = TFContinuousParameterModel(lognormal_factory, np.array([-2., 0.5]), len(branch_lengths), 100)\n",
    "#m = TFContinuousParameterModel(truncated_lognormal_factory, np.array([-1., 0.5, 0.1]), len(branch_lengths), 100)\n",
    "m.mode_match(branch_lengths)\n",
    "m.set_step_size()\n",
    "m.elbo_estimate(phylo_log_like, particle_count=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot(phylo_log_like, max_x=max_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.step_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = []\n",
    "trace = []\n",
    "window_size = 5\n",
    "stepsize_increasing_rate = 1.2\n",
    "stepsize_decreasing_rate = 1-5e-3\n",
    "stepsize_increasing = True\n",
    "best_elbo = -np.inf\n",
    "best_param_matrix = np.zeros(m.param_matrix.shape)\n",
    "for step in range(30):\n",
    "    if stepsize_increasing and step >= 2*window_size:\n",
    "        last_epoch = trace[-window_size:]\n",
    "        prev_epoch = trace[-2*window_size:-window_size]\n",
    "        if np.mean(last_epoch) < np.mean(prev_epoch):\n",
    "            np.copyto(m.param_matrix, best_param_matrix)\n",
    "            m.step_size /= 2\n",
    "            stepsize_increasing = False\n",
    "            print(\"\\nturning around decreasing\")\n",
    "    if stepsize_increasing:\n",
    "        m.step_size *= stepsize_increasing_rate\n",
    "    else:\n",
    "        m.step_size *= stepsize_decreasing_rate\n",
    "    print(m.step_size, end='')\n",
    "    m.sample_and_prep_gradients()\n",
    "    if not m.gradient_step(grad_phylo_log_like(m.z), history):\n",
    "        np.copyto(m.param_matrix, best_param_matrix)\n",
    "        m.step_size /= 2\n",
    "        stepsize_increasing = False\n",
    "        print(\"\\nturning around nan\")\n",
    "    trace.append(m.elbo_estimate(phylo_log_like, particle_count=500))\n",
    "    if trace[-1] > best_elbo:\n",
    "        best_elbo = trace[-1]\n",
    "        np.copyto(best_param_matrix, m.param_matrix)\n",
    "pd.Series(trace).plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = []\n",
    "window_size = 5\n",
    "for i in range(10, len(trace)):\n",
    "    trace = np.stack(history)[:i,-1]\n",
    "    last_epoch = trace[-window_size:]\n",
    "    prev_epoch = trace[-2*window_size:-window_size]\n",
    "    stat.append(np.mean(last_epoch) - np.mean(prev_epoch))\n",
    "    m.elbo_estimate(phylo_log_like, particle_count=500)\n",
    "pd.Series(stat).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.elbo_estimate(phylo_log_like, particle_count=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lognormal: -7153.707223708818"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* good: -2934.5720311158702\n",
    "* gamma with 200 steps: -2935.098012362405\n",
    "* lognormal with 200 steps: -2934.699239103927"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot(phylo_log_like, max_x=max_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.elbo_estimate(phylo_log_like, particle_count=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.stack(history)[:,-1], columns=[\"elbo\"]).plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot(phylo_log_like, max_x=max_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.elbo_estimate(phylo_log_like, particle_count=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as g:\n",
    "    tf_params = tf.constant(np.array([[-2., 0.5]]), dtype=tf.float32)\n",
    "    g.watch(tf_params)\n",
    "    q_distribution = lognormal_factory(tf_params)\n",
    "    mode_error = (0.025 - q_distribution.mode())**2\n",
    "    grad = g.gradient(mode_error, tf_params)\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.optimizer = optimizers.SGD_Server({\"params\": m.param_matrix.shape})\n",
    "# %%timeit -n 1\n",
    "pd.DataFrame({\"bl\": np.log(branch_lengths), \"y\": m.param_matrix[:,1]}).plot.scatter(x=\"bl\", y=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    history[-1] = np.append(history[-1], m.elbo_estimate(phylo_log_like, particle_count=500))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
