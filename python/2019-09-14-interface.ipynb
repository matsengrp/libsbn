{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "tf.enable_v2_behavior()\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optimizers\n",
    "import sbn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def log_like_with(branch_id: int, branch_length: float, grad=False):\n",
    "    saved_branch_length = branch_lengths[branch_id]\n",
    "    branch_lengths[branch_id] = branch_length\n",
    "    if grad:\n",
    "        _, log_grad = inst.branch_gradients()[0]\n",
    "        result = np.array(log_grad)[branch_id]\n",
    "    else:\n",
    "        result = np.array(inst.log_likelihoods())[0]\n",
    "    branch_lengths[branch_id] = saved_branch_length\n",
    "    return result\n",
    "\n",
    "def phylo_log_like(x_arr):\n",
    "    return np.array([log_like_with(2, x) for x in x_arr])\n",
    "\n",
    "def grad_phylo_log_like(x_arr):\n",
    "    return np.array([log_like_with(2, x, grad=True) for x in x_arr])\n",
    "\n",
    "x_vals = np.linspace(0, 0.3, 100)\n",
    "df = pd.DataFrame({\"x\": x_vals, \"y\": phylo_log_like(x_vals)})\n",
    "df.plot(x=\"x\", y=\"y\", kind=\"line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst = sbn.instance(\"charlie\")\n",
    "inst.tree_collection = sbn.TreeCollection(\n",
    "    [sbn.Tree.of_parent_id_vector([3, 3, 3])],\n",
    "    [\"mars\", \"saturn\", \"jupiter\"])\n",
    "inst.read_fasta_file('../data/hello.fasta')\n",
    "inst.make_beagle_instances(2)\n",
    "branch_lengths_extended = np.array(inst.tree_collection.trees[0].branch_lengths,\n",
    "                          copy=False)\n",
    "branch_lengths_extended[:] = np.array([0.1, 0.1, 0.3, 0.])\n",
    "# Here we are getting a slice that excludes the last (fake) element. \n",
    "# Thus we can just deal with the actual branch lengths.\n",
    "branch_lengths = branch_lengths_extended[:len(branch_lengths_extended)-1]\n",
    "\n",
    "def log_like_with(in_branch_lengths, grad=False):\n",
    "    global branch_lengths\n",
    "    saved_branch_lengths = branch_lengths.copy()\n",
    "    branch_lengths[:] = in_branch_lengths\n",
    "    if grad:\n",
    "        _, log_grad = inst.branch_gradients()[0]\n",
    "        result = np.array(log_grad)\n",
    "    else:\n",
    "        result = np.array(inst.log_likelihoods())[0]\n",
    "        branch_lengths[:] = saved_branch_lengths\n",
    "    return result\n",
    "\n",
    "def phylo_log_like(x_arr):\n",
    "    # TODO can do something better with some np mapping thing\n",
    "    return np.array([log_like_with(x) for x in x_arr])\n",
    "\n",
    "def grad_phylo_log_like(x_arr):\n",
    "    return np.array([log_like_with(x, grad=True) for x in x_arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_arr = np.array([[0.1, 0.1, x] for x in np.linspace(0, 0.3, 100)])\n",
    "df = pd.DataFrame({\"x\": x_arr[:,2], \"y\": phylo_log_like(x_arr)})\n",
    "df.plot(x=\"x\", y=\"y\", kind=\"line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamma_factory(params):\n",
    "    return tfp.distributions.Gamma(concentration=params[:,0], rate=params[:,1])\n",
    "\n",
    "def lognormal_factory(params):\n",
    "    return tfp.distributions.LogNormal(loc=params[:,0], scale=params[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def gamma_factory(params):\n",
    "    return tfp.distributions.Gamma(concentration=params[0], rate=params[1])\n",
    "\n",
    "def lognormal_factory(params):\n",
    "    return tfp.distributions.LogNormal(loc=params[0], scale=params[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 2.\n",
    "beta = 5.\n",
    "gamma = tfd.Gamma(concentration=alpha, rate=beta)\n",
    "\n",
    "def grad_log_like(x):\n",
    "    with tf.GradientTape() as g:\n",
    "        tf_x = tf.constant(x, dtype=tf.float32)\n",
    "        g.watch(tf_x)\n",
    "        return g.gradient(gamma.log_prob(tf_x), tf_x).numpy()\n",
    "\n",
    "def log_like(x):\n",
    "    return gamma.log_prob(x)\n",
    "    \n",
    "grad_log_like(np.array([0.3, 0.2]))\n",
    "log_like(np.array([0.3, 0.2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFContinuousParameterModel:\n",
    "    def __init__(self, q_factory, initial_params, params_count, particle_count, step_size=0.01):\n",
    "        assert initial_params.ndim == 1\n",
    "        self.q_factory = q_factory\n",
    "        self.param_matrix = np.full((params_count, len(initial_params)), initial_params)\n",
    "        #self.param_matrix = np.copy(initial_params)\n",
    "        self.particle_count = particle_count\n",
    "        self.step_size = step_size\n",
    "        # The current stored sample.\n",
    "        self.x = None\n",
    "        # The gradient of x with respect to the parameters of q.\n",
    "        self.grad_x = None\n",
    "        # The stochastic gradient of log sum q for x.\n",
    "        self.grad_log_sum_q = None\n",
    "        \n",
    "        \n",
    "    def sample(self):\n",
    "        with tf.GradientTape(persistent=True) as g:\n",
    "            tf_params = tf.constant(self.param_matrix, dtype=tf.float32)\n",
    "            g.watch(tf_params)\n",
    "            q_distribution = self.q_factory(tf_params)\n",
    "            tf_x = q_distribution.sample(self.particle_count)\n",
    "            q_term = tf.math.reduce_sum(tf.math.log(q_distribution.prob(tf_x)))\n",
    "        self.x = tf_x.numpy()\n",
    "        # The Jacobian is laid out as particles x edges x edges x params.\n",
    "        self.grad_x = np.sum(g.jacobian(tf_x, tf_params).numpy(), axis=2)\n",
    "        self.grad_log_sum_q = g.gradient(q_term, tf_params).numpy()\n",
    "        del g  # Should happen anyway but being explicit to remember.\n",
    "        return self.x\n",
    "    \n",
    "    \n",
    "    def clear_sample(self):\n",
    "        self.x = None\n",
    "        self.grad_x = None\n",
    "        self.grad_log_sum_q = None\n",
    "\n",
    "    \n",
    "    def elbo_gradient_using_current_sample(self, grad_log_p_x):\n",
    "        assert self.grad_x is not None\n",
    "        # Chain rule for the first term.\n",
    "        unnormalized_result = np.matmul(grad_log_p_x, self.grad_x) - self.grad_log_sum_q\n",
    "        return unnormalized_result / self.particle_count\n",
    "    \n",
    "    \n",
    "    def elbo(self, target_log_like, max_x = 0.5):\n",
    "        q_distribution = self.q_factory(self.param_matrix)\n",
    "        min_x = max_x/100\n",
    "        x_vals = np.linspace(min_x, max_x, 100)\n",
    "        p_log_likes = target_log_like(x_vals)\n",
    "        q_log_probs = q_distribution.log_prob(x_vals)\n",
    "        return np.sum(sp.special.softmax(q_log_probs) * (p_log_likes - q_log_probs))\n",
    "\n",
    "    \n",
    "    def gradient_step(self, grad_log_p_x, history = None):\n",
    "        grad = self.elbo_gradient_using_current_sample(grad_log_p_x)\n",
    "        self.param_matrix += self.step_size * grad\n",
    "        self.clear_sample()\n",
    "        if history is not None:\n",
    "            history.append(np.concatenate([self.param_matrix]))\n",
    "    \n",
    "    \n",
    "    def plot(self, target_log_like, max_x = 0.5):\n",
    "        min_x = max_x/100\n",
    "        x_vals = np.linspace(min_x, max_x, 100)\n",
    "        q_distribution = self.q_factory(self.param_matrix)\n",
    "        df = pd.DataFrame({\n",
    "            \"x\": x_vals, \n",
    "            \"target\": sp.special.softmax(target_log_like(x_vals)),\n",
    "            \"fit\": sp.special.softmax(q_distribution.log_prob(x_vals).numpy())})\n",
    "        df.plot(x=\"x\", y=[\"target\", \"fit\"], kind=\"line\", \n",
    "                title=q_distribution._name+\" \"+str(self.param_matrix))\n",
    "\n",
    "    \n",
    "#m = TFContinuousParameterModel(gamma_factory, np.array([2., 12.]), 3, 100, step_size=0.05)\n",
    "m = TFContinuousParameterModel(lognormal_factory, np.array([-2., 0.5]), 3, 5)\n",
    "print(m.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phylo_log_like(m.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(grad_log_p_x, self.grad_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.param_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.grad_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(grad_log_p_x, self.grad_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in m.grad_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot(phylo_log_like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = []\n",
    "for _ in range(100):\n",
    "    m.sample()\n",
    "    m.gradient_step(grad_phylo_log_like(m.x), history)\n",
    "    history[-1] = np.append(history[-1], m.elbo(phylo_log_like))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot(phylo_log_like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
