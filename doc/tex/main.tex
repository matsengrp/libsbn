\documentclass{article}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{bm}
\usepackage{showlabels}

\newcommand{\E}{\mathbb{E}}
\newcommand{\psp}{\slash\!\!\slash}
\newcommand{\qSplit}{q^\textsf{S}\hspace{-1pt}}
\newcommand{\gSplit}{g^\textsf{S}\hspace{-1pt}}
\newcommand{\qPSP}{q^\textsf{PSP}\hspace{-1pt}}
\newcommand{\gPSP}{g^\textsf{PSP}\hspace{-1pt}}
\newcommand{\softmax}{\operatorname*{softmax}}
\newcommand{\PCSS}{\operatorname{PCSS}}

\title{libsbn implementation notes}
\author{Team Phylo VI}

\begin{document}
\maketitle


\section*{Introduction}

\subsection*{Notation}

In the 2019 ICLR paper, we used $q$ for branch lengths and $Q$ for the variational approximation.
Here we will use $\bm \theta$ for branch lengths and $q$ for the variational approximation.
This $p$ and $q$ corresponds to most of the literature on VI, and allows us to use $Q$ for the Markov transition matrix.


\subsection*{Setup}

Define
\[
    f_{\bm{\phi},{\bm{\psi}}}(\tau, \bm \theta) :=
    \frac{p(\bm{Y} \mid \tau, \bm{\theta}) \, p(\tau, \bm{\theta})}
    {q_{\bm{\phi}}(\tau)\, q_{\bm{\psi}}(\bm{\theta}|\tau)}
    = \frac{p(\bm{Y}, \tau, \bm{\theta})}
    {q_{\bm{\phi}}(\tau)\, q_{\bm{\psi}}(\bm{\theta}|\tau)}
\]

We want to maximize the ELBO of our variational parameterization $q$
\[
L(\bm{\phi},{\bm{\psi}}) :=
\mathbb{E}_{q_{\bm{\phi},{\bm{\psi}}}(\tau, \bm{\theta})}
\log\left( f_{\bm{\phi},{\bm{\psi}}}(\tau, \bm \theta) \right) \leq \log p(\bm{Y}).
\]
over $\phi$ and $\psi$.
The discrete part of the variational distribution is given by an SBN distribution $q_{\bm\phi}$, while $q_{\bm\psi}$ is the scalar density.

Now, $q_{\bm{\phi}}(\tau)$ is going to be the sum over rootings of the rooted SBN probability of $\tau^j$:
\[
    q_\phi(\tau) = \sum_{b \in \tau} q_\phi(\rho_b(\tau)).
\]
where $\rho_b$ here means to root $\tau$ at branch $b$ and $q_\phi(\rho_b(\tau))$ is an ``overloaded'' definition of $q_\phi$ meaning the rooted SBN probability.

We continue to overload $q_\phi$.
Let $q_\phi(b \slash \tau)$ be the probability of the rootsplit for $\tau$ rooted at $b$, and
$q_\phi(s)$ be the probability of a PCSS $s$.
Now we have
\begin{equation}
    q_\phi(\rho_b(\tau)) = q_\phi(b \slash \tau) \, \prod_{s \in \PCSS(\rho_b(\tau))} q_\phi(s).
    \label{eq:qRooted}
\end{equation}

We will store the $\phi$ parameters in pre-softmax form, so

\[
    q_\phi(b \slash \tau) = \left[ \softmax_{b' \in \tau} \phi_{b' \slash \tau} \right]_b .
\]
and similarly for $q_\phi(s)$, but where the softmax is over other subsplits $s'$ with the same parent as $s$.


\section*{Estimating SBN parameters via the EM algorithm for unrooted trees}

We provide further details on the EM algorithm used for estimating the SBN probabilities.
The parameterization is somewhat implicit in the original derivation provided in \cite{Zhang2018-mm}.
For clarity, we make explicit the parameters of the conditional probability table $\bm{\phi}$ in this version.
For unrooted trees, the root split values are latent variables and the observed variables are the remaining splits given the value of the root split.
In the terminologies of the EM algorithm, we have a latent variable $S_1$ whose distribution we denote, $P(S_1 | \bm{\phi})$ and the complete data likelihood given by
\begin{equation}
    P(S_1, T^u | \bm{\phi}) = P(S_1 | \bm{\phi}) P(T^u | S_1, \bm{\phi}) = P(S_1 | \bm{\phi}) \prod\limits_{i > 1} P(S_i | S_{\pi(i)}, \bm{\phi}).
\end{equation}
The E-step computes an expectation of the log of the complete data likelihood over the latent variable $S_1$ and the M-step maximizes it over $\bm{\phi}$.
It is customary to refer to the output of the E-step as the Q-function (or Q-score).
Note that in this section, $Q$ will not denote the rate matrix of a phylogenetic model but rather the Q-function of the EM algorithm (we default back to $Q$ being the rate matrix in all other sections).
The Q-function for the $n$-th iteration of the EM algorithm is given by
\begin{align*}
    Q(\bm{\phi}, \bm{\phi}^{(n-1)}) &= \E_{S_1 \sim P(S_1 | \bm{\phi}^{(n-1)})}\left[ \log P(S_1, T^u | \bm{\phi}) \right] \\
    &= \sum\limits_{s_1} P(S_1 = s_1 | \bm{\phi}^{(n-1)}) \log P(S_1, T^u | \bm{\phi}).
\end{align*}
The M-step maximizes this Q-function over $\bm{\phi}$. After M-step, we have a value $\bm{\phi}^{(n)}$.
The Q-score $Q(\theta^{(n)}, \theta^{(n-1)})$ is assured to be at least as large as $Q(\theta^{(n-1)}, \theta^{(n-1)})$, by definition of maximization.
The key idea behind the EM algorithm is that the new value $\bm{\phi}^{(n)}$ results in at least this much increase in the log likelihood of the data:
\begin{equation*}
    0 \leq Q(\bm{\phi}^{(n)}, \bm{\phi}^{(n-1)}) - Q(\bm{\phi}^{(n-1)}, \bm{\phi}^{(n-1)}) \leq \log P(T^u | \bm{\phi}^{(n)}) - \log P(T^u | \bm{\phi}^{(n-1)}).
\end{equation*}
Therefore, a suitable surrogate function to track the convergence of EM algorithm and to test for correctness of the implementation of the EM algorithm is $\log P(T^u | \bm{\phi}^{(n)})$, which can be computed upon completion of one iteration of the EM algorithm. This is easy to compute since,
\begin{equation*}
    \log P(T^u | \bm{\phi}^{(n)}) = \log \sum\limits_{s_1} P(T^u | S_1 = s_1, \bm{\phi}^{(n)}) P(S_1 = s_1 | \bm{\phi}^{(n)}).
\end{equation*}
To be precise, since we have a collection of trees $\mathcal{D}^u = \{T^u_k\}_{k=1}^{K}$ as our data and assuming independence across the trees in the collection, we need to compute and monitor,
\begin{align*}
    \log P(T^u_1, ..., T^u_K | \bm{\phi}^{(n)}) =& \log \prod\limits_{k=1}^{K} P(T^u_k | \bm{\phi}^{(n)}) \\
    =& \sum\limits_{k=1}^{K} \log P(T^u_k | \bm{\phi}^{(n)}) \\
    =& \sum\limits_{k=1}^{K} \log \sum\limits_{s_1} P(S_1 = s_1 | \bm{\phi}^{(n)}) P(T^u_k | S_1 = s_1, \bm{\phi}^{(n)}).
\end{align*}
\textbf{This is the quantity called the score in the code, in contrast to the original paper.}

We complete this section by providing details on the regularized EM algorithm.
The regularization from the view of Bayesian statistics is to place a Dirichlet distribution as prior on the parameters $\bm{\phi}_{.|t} = \{\phi_{s|t}\}_{s \in \mathbb{C}_{.|t}}$.
We will denote the prior distribution by, $P(\bm{\phi}_{.|t} | \bm{\alpha}_{.|t})$.
Note that we actually have a collection of such prior distributions, one for each parent subsplit.
For convenience of notation, we can assume that $t = \emptyset$ for the root split.
The hyperparameter vector $\bm{\alpha}_{.|t}$ is given by an empirical Bayes estimate $\alpha_{s|t} = \alpha \tilde{m}_{s|t}$ where $\alpha > 0$ is a global parameter and $\{\tilde{m}_{s|t}\}$ are the sample frequencies, i.e., the number of times we observed $s|t$.
With the Bayesian formulation, the Q-function is expectation of the log of the posterior distribution rather than log of complete data likelihood w.r.t. the latent rooting $S_1 \sim P(S_1 | \bm{\phi}^{(n-1)})$.
It takes on the following form:
\begin{align*}
    \tilde{Q}(\bm{\phi}, \bm{\phi}^{(n-1)}) &= \E_{S_1 \sim P(S_1 | \bm{\phi}^{(n-1)})}\left[ \log P(\bm{\phi} | S_1, T^u, \alpha) \right] \\
    &= \E_{S_1 \sim P(S_1 | \bm{\phi}^{(n-1)})}\left[ \log \frac{P(S_1, T^u | \bm{\phi}) P(\bm{\phi} | \alpha))}{P(S_1, T^u | \alpha)} \right] \\
    &\propto \E_{S_1 \sim P(S_1 | \bm{\phi}^{(n-1)})}\left[ \log P(S_1, T^u | \bm{\phi}) P(\bm{\phi} | \alpha) \right] \\
    &= \E_{S_1 \sim P(S_1 | \bm{\phi}^{(n-1)})}\left[ \log P(S_1 | \bm{\phi}_{1}) P(\bm{\phi}_1 | \alpha) + \sum\limits_{s|t \in \mathbb{C}_{.|t}} \log P(s | t, \bm{\phi}_{.|t}) P(\bm{\phi}_{.|t} | \alpha) \right],
    % &= \E_{S_1 \sim P(S_1 | \bm{\phi}^{(n-1)})}\left[ \log P(S_1, T^u | \bm{\phi}) \right] + \E_{S_1 \sim P(S_1 | \bm{\phi}^{(n-1)})}\left[ \log P(\bm{\phi} | \alpha) \right] \\
    % &= \sum\limits_{s_1} P(S_1 = s_1 | \bm{\phi}^{(n-1)}) \log P(S_1 = s_1 | \bm{\phi}) P(T^u | S_1 = s_1, \bm{\phi}) + \log P(\bm{\phi} | \alpha) \\
    % &= Q(\bm{\phi}, \bm{\phi}^{(n-1)}) + \log P(\bm{\phi} | \alpha),
\end{align*}
where
\begin{equation*}
    \log \left( P(s | t, \bm{\phi}_{.|t})P(\bm{\phi}_{.|t} | \alpha) \right) = \log(\phi_{s|t}) + \alpha \tilde{m}_{s|t} \log(\phi_{s|t}),
\end{equation*}
using the notation $\phi_1 = P(S_1)$ and $\phi_{s|t} = P(s | t, \bm{\phi})$. Therefore, the final form for $\tilde{Q}$ is given by,
\begin{equation*}
    \tilde{Q}(\bm{\phi}, \bm{\phi}^{(n-1)}) = Q(\bm{\phi}, \bm{\phi}^{(n-1)}) + \sum\limits_{s_1 \in \mathbb{C}_r} \alpha \tilde{m}_{s_1}^u \log P(s_1 | \bm{\phi}) + \sum\limits_{s|t \in \mathbb{C}_{.|t}} \alpha \tilde{m}_{s|t}^u \log P(s | t, \bm{\phi}).
\end{equation*}

\section*{Gradient with respect to SBN parameters $\phi$}

Assume that $(\tau^{1:K}, \bm \theta^{1:K})$ are drawn from the joint distribution on $(\tau^i, \bm \theta^i)$ given by $q_{\bm{\phi},\bm{\psi}}(\tau, \bm{\theta})$.
The stochastic lower bound with these $K$ samples is
\[
    \hat{L}^K(\bm{\phi},{\bm{\psi}}) :=
    \log\left( \frac{1}{K} \sum_{i=1}^K f_{\bm{\phi},{\bm{\psi}}}(\tau^i, \bm \theta^i) \right).
\]

Set
\[
    \tilde{w}^j :=
    \frac{f_{\bm{\phi},{\bm{\psi}}}(\tau^j, \bm \theta^j)}
    {\sum_{i=1}^K f_{\bm{\phi},{\bm{\psi}}}(\tau^i, \bm \theta^i)}.
\]

Following the first section of Supplement A of the 2019 ICLR paper, we have the naive gradient estimate of the ELBO:
\[
\nabla_{{\bm{\phi}}}L^k({\bm{\phi}},{\bm{\psi}}) = \mathbb{E}_{q_{{\bm{\phi}},{\bm{\psi}}}(\tau^{1:K},\; {\bm{\theta}}^{1:K})} \sum_{j=1}^K \left(\hat{L}^K({\bm{\phi}},{\bm{\psi}}) - \tilde{w}^j\right) \nabla_{\bm{\phi}} \log q_{\bm{\phi}}(\tau^j).
\]

To get the VIMCO gradient, we follow equation 10 of~\cite{Mnih2016-ec}, noting that their $\nabla_\theta \, \log f(x, h^j)$ is $-\nabla_{\bm{\phi}} \log q_{\bm{\phi}}(\tau^j)$ in our case.
This gives (5) of the 2019 ICLR paper, which in our notation is
\[
\nabla_{\bm{\phi}} L^K(\bm{\phi},{\bm{\psi}}) = \mathbb{E}_{q_{\bm{\phi},{\bm{\psi}}}(\tau^{1:K},\;\bm{\theta}^{1:K})}\sum_{j=1}^K\left(\hat{L}_{j|-j}^K(\bm{\phi},{\bm{\psi}})-\tilde{w}^j\right)\nabla_{\bm{\phi}}\log q_{\bm{\phi}}(\tau^j)
\]
where
\[
\hat{L}_{j|-j}^K(\bm{\phi},{\bm{\psi}}) :=  \hat{L}^K(\bm{\phi},{\bm{\psi}}) - \log\frac1K\left(\sum_{i\neq j}f_{\bm{\phi},{\bm{\psi}}}(\tau^i,\bm{\theta}^i) + \hat{f}_{\bm{\phi},{\bm{\psi}}}(\tau^{-j},\bm{\theta}^{-j})\right)
\]
is the per-sample local learning signal, with $\hat{f}_{\bm{\phi},{\bm{\psi}}}(\tau^{-j},\bm{\theta}^{-j})$ being some estimate of $f_{\bm{\phi},{\bm{\psi}}}(\tau^j,\bm{\theta}^j)$ for sample $j$ using the rest of samples (e.g., the geometric mean).

We will calculate $\nabla_{\bm \phi} \log q_{\bm \phi}(\tau^j)$ as the ratio ${\nabla_{\bm \phi} \, q_{\bm{\phi}}(\tau^j)} / {q_{\bm{\phi}}(\tau^j)}$.

To calculate $\nabla_{\bm \phi} \, q_{\bm{\phi}}(\tau)$, we take derivatives of the sum across rootings, where each derivative is just that of a product as per~\eqref{eq:qRooted}.


\section*{Gradient with respect to scalar model parameters $\psi$}

This section concerns taking the derivative of~\eqref{eq:L} with respect to a $\psi_i$.

We will use $g_{\bm{\psi}}(\bm{\epsilon}|\tau)$ to designate the ``reparametrization function'' such that we can get a sample from $q_{\bm\psi}$ by applying $g_{\bm{\psi}}(\bm{\epsilon}|\tau)$ to some variates $\bm\epsilon$ drawn from a fixed distribution to get $\bm\theta$.
Said another way, each entry of $\bm{\epsilon}$ can be obtained by applying the relevant standardization function to a sample from $q_{\bm\psi}$.
We will use component-wise notation $\theta_b = g_{b, \psi}(\bm\epsilon|\tau)$ for branch $b$ of the topology $\tau$.
So, applying the reparametrization trick we get
\begin{equation}
L(\bm{\phi},{\bm{\psi}}) = \mathbb{E}_{
    q_{\bm{\phi}}(\tau,\bm{\epsilon})}
    \log\left(
        \frac
        {p(\bm{Y},\tau,g_{\bm{\psi}}(\bm{\epsilon}|\tau))}
        {q_{\bm{\phi}}(\tau)q_{\bm{\psi}}(g_{\bm{\psi}}(\bm{\epsilon}|\tau)|\tau)}
    \right).
\label{eq:L}
\end{equation}

Taking the derivative of the numerator of~\eqref{eq:L} with respect to a $\psi_i$ gives, in the general case
\begin{equation}
    \sum_b
    \frac{\partial \log p(\bm{Y} , \tau, \bm\theta)}{\partial \theta_b} \,
    \frac{\partial g_{b,\bm\psi}(\bm\epsilon | \tau)}{\partial \psi_i}.
    \label{eq:dlogpdPsi}
\end{equation}
Typically a variational parameter will appear in the expression for at most one branch, thus in this case at most one term of this sum will be nonzero.

For the left hand side, we have
\[
p(\bm{Y},\tau,\bm\theta) =
p(\bm{Y}|\tau,\bm\theta) \, p(\bm\theta | \tau) \, p(\tau).
\]
So
\begin{equation*}
\frac{\partial \log p(\bm{Y}, \tau, \bm\theta)}{\partial \theta_b} =
\frac{\partial \log p(\bm{Y} | \tau, \bm\theta)}{\partial \theta_b}
+
\frac{\partial \log p(\bm\theta | \tau)}{\partial \theta_b}
\end{equation*}
The left hand term of the sum is the standard log phylogenetic gradient, while the right hand term is the derivative of the log branch length prior with respect to a specific branch.

In the case where the branch length prior $p(\bm\theta | \tau)$ is a product of terms $\prod_b p_b(\theta_b)$ this becomes very simple:
\[
    \frac{\partial \log p(\bm\theta | \tau)}{\partial \theta_b} =
    \frac{\partial \log p_b(\theta_b)}{\partial \theta_b}.
\]

To keep things compact, though, we'll assume that this is all understood and will stick with the unexpanded form of the unnormalized posterior
\[
    \frac{\partial \log p(\bm{Y}, \tau, \bm\theta)}{\partial \theta_b}
\]
in the development below.

For the denominator of \eqref{eq:L} we only need to worry about the $q_{\bm\psi}$ term.
We will assume that $q_{\bm\psi}(\bm\theta | \tau)$ factors as a product of terms across branches of the topology, so
\[
    - \log q_{\bm{\psi}}(g_{\bm{\psi}}(\bm{\epsilon}|\tau)|\tau) =
    - \sum_b \log q_{b, \bm{\psi}}(g_{b, \bm{\psi}}(\bm{\epsilon}|\tau)|\tau).
\]

So, we get the derivative of $L(\bm\phi, \bm\psi)$ with respect to $\psi_i$ being the sum over $b$ of:
\begin{equation}
    \frac{\partial \log p(\bm{Y}, \tau, \bm\theta)}{\partial \theta_b}
    \frac{\partial g_{b,\bm\psi}(\bm\epsilon | \tau)}{\partial \psi_i}
    - \frac{\partial \log q_{b, \bm\psi}(g_{b, \bm\psi}(\bm\epsilon|\tau)|\tau)}{\partial \psi_i}.
    \label{eq:dLdPsi}
\end{equation}

\subsubsection*{Split-based parametrization:}
First consider the split-based parametrization given in the ``simple independent approximation'' section of the 2019 ICLR paper.
If we use $b \slash \tau$ to mean the split given by branch $b$ of topology $\tau$, then the parametrization is
\[
q_{b, \bm{\psi}}(\theta_b | \tau) := \qSplit(\theta_b; \psi_{b \slash \tau})
\qquad
g_{b,\bm\psi}(\bm\epsilon | \tau) := \gSplit(\epsilon_b; \psi_{b \slash \tau}).
\]
for some function $\qSplit(\theta; \psi)$ with corresponding $\gSplit(\epsilon; \psi)$.

To implement this, index $\bm\psi$ by splits.
To get the gradient of $L(\bm\phi, \bm\psi)$ with respect to $\bm\psi$, we can start with a zero vector then loop over the branches $b$, and for each one incrementing the $b \slash \tau$ entry by
\begin{equation*}
    \frac{\partial \log p(\bm{Y}, \tau, \bm\theta)}{\partial \theta_b}
    \frac{\partial \gSplit(\epsilon_b; \psi_{b \slash \tau})}{\partial \psi_{b \slash \tau}}
    - \frac{\partial \log \qSplit(\gSplit(\epsilon_b; \psi_{b \slash \tau}); \psi_{b \slash \tau})}{\partial \psi_{b \slash \tau}}.
\end{equation*}


\subsubsection*{Primary subsplit pair parametrization:}
Next we consider the ``primary subsplit pair'' (PSP) parametrization.
We use $b \psp \tau$ to mean the primary subsplit pair given by branch $b$ of topology $\tau$: the parameters of the approximating distribution for a PSP $b \psp \tau$ are the sum of the split corresponding to the split $b \slash \tau$ and the two subsplits refining $b \slash \tau$.

We will use $\bm\psi_{b \psp \tau}$ to represent a vector of the entries of $\bm\psi$ for the three variables contributing to the PSP.
We will use $\bm\epsilon_{b \psp \tau}$ to denote the standardized versions of the draws from the random variables used to parameterize $b \psp \tau$.
The parametrization is formally very similar:
\[
q_{b, \bm{\psi}}(\theta_b | \tau) := \qPSP(\theta_b; \bm\psi_{b \psp \tau})
\qquad
g_{b,\psi}(\bm\epsilon | \tau) := \gPSP(\bm\epsilon_{b \psp \tau}; \bm\psi_{b \psp \tau}).
\]
So we can do a similar procedure as before, in which we loop over all branches $b$ of $\tau$, but now we also loop over (sub)splits $\varsigma \in b \psp \tau$, and for each one incrementing the corresponding entry of the gradient by:
\begin{equation}
    \frac{\partial \log p(\bm{Y}, \tau, \bm\theta)}{\partial \theta_b}
    \frac{\partial \gPSP(\bm\epsilon_{b \psp \tau}; \bm\psi_{b \psp \tau})}{\partial \psi_{\varsigma}}
    - \frac{\partial \log \qPSP(\gPSP(\bm\epsilon_{b \psp \tau}; \bm\psi_{b \psp \tau}); \bm\psi_{b \psp \tau})}{\partial \psi_{\varsigma}}.
    \label{eq:PSPgradComponent}
\end{equation}
The fact that this is a sum across branches is justified by our assumption that $q$ factors across branches of the tree.

\subsubsection*{Generalization:}
This general recipe works given
\begin{enumerate}
    \item a variational distribution for branch lengths $q$ that factors as a product across branches
    \item a matching reparametrization function $g$
    \item a ``branch representation:'' a means of mapping a branch $b$ to the variables $\psi_\varsigma$ involved in its parametrization
    \item a means of taking the relevant derivatives
\end{enumerate}

We could develop some new notation for the general case, but we might as well just look at \eqref{eq:PSPgradComponent}, and replace $b \psp \tau$ in our minds with ``the variables $\varsigma$ relevant for branch $b$ of topology $\tau$.''


\subsection*{Families of scalar variational distributions}
\subsubsection*{Split-based Log-normal:}
If $q$ is log-normal, we take $\psi_\varsigma$ as being the location-scale pair $(\mu_\varsigma, \sigma_\varsigma)$,
\[
\log q(\theta; \psi_\varsigma) := c - \log \theta - \log \sigma_\varsigma - \frac{(\log \theta - \mu_\varsigma)^2}{2 \sigma_\varsigma^2}.
\]
for a constant $c$.
Taking
\begin{equation}
g(\epsilon; \psi_\varsigma) := \exp(\mu_\varsigma + \sigma_\varsigma \epsilon),
\label{eq:gLogNorm}
\end{equation}
we have
\begin{align*}
\log q(g(\epsilon; \psi_\varsigma); \psi_\varsigma)
& := c - (\mu_\varsigma + \sigma_\varsigma \epsilon)
    - \log \sigma_\varsigma
    - \frac{(\mu_\varsigma + \sigma_\varsigma \epsilon - \mu_\varsigma)^2}{2 \sigma_\varsigma^2} \\
& := c - \mu_\varsigma - \sigma_\varsigma \epsilon - \log \sigma_\varsigma - \frac{\epsilon^2}{2}.
\end{align*}
Thus,
\begin{equation}
    \frac{\partial \log q(g(\epsilon; \psi_\varsigma); \psi_\varsigma)}{\partial \mu_\varsigma} = -1
    \qquad
    \frac{\partial \log q(g(\epsilon; \psi_\varsigma); \psi_\varsigma)}{\partial \sigma_\varsigma} = -\epsilon - \frac{1}{\sigma_\varsigma}.
    \label{eq:dlogqgdPsi}
\end{equation}
Also,
\begin{equation}
    \frac{\partial g(\epsilon; \psi_\varsigma)}{\partial \mu_\varsigma} = g(\epsilon; \psi_\varsigma)
    \qquad
    \frac{\partial g(\epsilon; \psi_\varsigma)}{\partial \sigma_\varsigma} = g(\epsilon; \psi_\varsigma) \cdot \epsilon \, .
    \label{eq:dgdPsi}
\end{equation}


\subsubsection*{PSP Log-normal:}

The lognormal PSP parameterization uses the sum of the parameters across subsplits of the PSP for each component of the Log-normal parametrization:
\[
\qPSP(\theta; \bm\psi_{b \psp \tau}) :=
q\left(\theta; \left(
        \textstyle\sum_{\varsigma \in b \psp \tau} \mu_\varsigma,
        \textstyle\sum_{\varsigma \in b \psp \tau} \sigma_\varsigma
    \right)\right)
\]
\[
\gPSP(\theta; \bm\psi_{b \psp \tau}) :=
    \exp \left(
        \textstyle\sum_{\varsigma \in b \psp \tau} \mu_\varsigma +
        \epsilon
        \textstyle\sum_{\varsigma \in b \psp \tau} \sigma_\varsigma
    \right)
\]
Note that there is a single $\epsilon$ for each $b$ (rather than having a vector of such things).
The derivatives follow like in the split-based case by replacing $\mu_\varsigma$ and $\sigma_\varsigma$ with the relevant sum of parameters.
They come out the same except for
\[
    \frac
    {\partial \log \qPSP(\gPSP(\bm\epsilon_{b \psp \tau}; \bm\psi_{b \psp \tau}); \bm\psi_{b \psp \tau})}
    {\partial \sigma_{\varsigma}}
    =
    - \epsilon
    - \frac{1}{\textstyle\sum_{\varsigma' \in b \psp \tau} \sigma_{\varsigma'}}
\]
when $\varsigma \in b \psp \tau$ (and zero otherwise), which is a little different than $\frac{1}{\sigma_\varsigma}$ as for the simpler split-based distribution above.

Coding-wise it comes out very simply.
Loop over the particles, each of which has a branch representation.
For each branch (which can be considered as a split), take the sum of the coefficients given in its branch representation to get a split-based parameterization of the lognormal.
Calculate the derivatives as for the usual split-based lognormal.
Then set the derivative for each PSP component that is participating in a given split to the corresponding element of the split-based lognormal derivative.

% \begin{figure}[h]
% \centering
% \includegraphics[width=0.35\textwidth]{figures/subsplit.pdf}
% \caption{\
% A subsplit structure.
% }
% \label{fig:subsplit}
% \end{figure}

\nocite{vbpi}

\bibliographystyle{plain}
\bibliography{main}

\end{document}
